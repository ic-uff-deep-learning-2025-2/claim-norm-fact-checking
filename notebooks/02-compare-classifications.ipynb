{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c2cb0e",
   "metadata": {},
   "source": [
    "# 02 - Compare Classifications Made by Different Models on Fact-Checking Task\n",
    "\n",
    "This notebook is responsible for performing a comparative analysis of the classifications made by different models on the fact-checking task. It loads the results from various fine-tuned transformer models and compares their classifications against each other and against the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc1d36",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f1d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Third Party\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8c176",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "293ac037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging (safe for notebook re-runs)\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "if not root_logger.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "else:\n",
    "    # Avoid duplicate handlers when re-running notebook cells: just set levels\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    for h in root_logger.handlers:\n",
    "        h.setLevel(logging.INFO)\n",
    "    # Optionally disable propagation to avoid duplicate output from external loggers\n",
    "    root_logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629ace1",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e00814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Constants\n",
    "TIMESTAMP = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Dataset Constants\n",
    "DATASET_NAME = \"faketweetbr\"  # [\"faketweetbr\", \"fakebr\"]\n",
    "\n",
    "# Model Constants\n",
    "MODEL_NAME = \"neuralmind/bert-large-portuguese-cased\"  # [\"FacebookAI/xlm-roberta-large\", \"neuralmind/bert-large-portuguese-cased\"]\n",
    "\n",
    "# Paths Constants\n",
    "RESULTS_PATH = (\n",
    "    f\"../data/{DATASET_NAME}/classification_results/{MODEL_NAME.split('/')[-1]}/\"\n",
    ")\n",
    "CLASSIFICATIONS_PATH = os.path.join(RESULTS_PATH, \"classifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a2bde5",
   "metadata": {},
   "source": [
    "### Combine Classification Results from Different Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21719681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:43:00,865 - INFO - Found 9 result files in ../data/faketweetbr/classification_results/xlm-roberta-large/classifications\n",
      "2025-12-03 11:43:00,885 - INFO - Selected 3 latest result files for combination: ['claim-normalization_gpt-5-nano_2025-10-13_10-24-48_test-set-eval_2025-11-23_10-41-24.csv', 'claim-normalization_gpt-5_2025-10-13_10-23-02_test-set-eval_2025-11-23_10-56-43.csv', 'original_test-set-eval_2025-11-23_03-50-32.csv']\n",
      "2025-12-03 11:43:00,917 - INFO - Combined results DataFrame shape: (45, 6)\n",
      "2025-12-03 11:43:00,925 - INFO - Saved combined results to ../data/faketweetbr/classification_results/xlm-roberta-large/combined_classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Name of the combined results file\n",
    "combined_results_file = \"combined_classification_results.csv\"\n",
    "\n",
    "# List all result files\n",
    "result_files = [f for f in os.listdir(CLASSIFICATIONS_PATH) if f.endswith(\".csv\")]\n",
    "logging.info(f\"Found {len(result_files)} result files in {CLASSIFICATIONS_PATH}\")\n",
    "\n",
    "# Get only the latest file of each type\n",
    "latest_files = []\n",
    "\n",
    "for file_name in result_files:\n",
    "    task_type = file_name.split(\"_\")[0]\n",
    "    model_variant = (\n",
    "        file_name.split(\"_\")[1] + \"_\" + file_name.split(\"_\")[2]\n",
    "        if task_type != \"original\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Filter files of the same type\n",
    "    same_type_files = [\n",
    "        f\n",
    "        for f in result_files\n",
    "        if f.startswith(task_type)\n",
    "        and (\n",
    "            f.split(\"_\")[1] + \"_\" + file_name.split(\"_\")[2] == model_variant\n",
    "            if model_variant\n",
    "            else True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Get the latest file based on timestamp in the filename\n",
    "    latest_file = max(\n",
    "        same_type_files,\n",
    "        key=lambda x: pd.to_datetime(\n",
    "            x.split(\"_test-set-eval_\")[-1].replace(\".csv\", \"\"),\n",
    "            format=\"%Y-%m-%d_%H-%M-%S\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if latest_file not in latest_files:\n",
    "        latest_files.append(latest_file)\n",
    "\n",
    "# Use only the latest files for combination\n",
    "result_files = latest_files\n",
    "logging.info(f\"Selected {len(result_files)} latest result files for combination: {result_files}\")\n",
    "\n",
    "# Create DataFrame to hold all results\n",
    "all_results_df = pd.DataFrame(\n",
    "\t\tcolumns=[\"custom_id\", \"original_label\", \"original_text\"]\n",
    ")\n",
    "\n",
    "for file_name in result_files:\n",
    "    # Load individual results file\n",
    "    results_df = pd.read_csv(os.path.join(CLASSIFICATIONS_PATH, file_name))\n",
    "\n",
    "    # Determine task type and model variant from file name\n",
    "    task_type = file_name.split(\"_\")[0].replace(\"-\", \"_\")\n",
    "    model_variant = file_name.split(\"_\")[1] if task_type != \"original\" else None\n",
    "    column_prefix = f\"{task_type}_{model_variant}\" if model_variant else task_type\n",
    "\n",
    "    # If all_results_df is empty, initialize it with the first results_df\n",
    "    if all_results_df.empty:\n",
    "        all_results_df[\"custom_id\"] = results_df[\"custom_id\"]\n",
    "        all_results_df[\"original_label\"] = results_df[\"original_label\"]\n",
    "\n",
    "    # For the \"original\" task, also add the original text\n",
    "    if task_type == \"original\":\n",
    "        all_results_df[\"original_text\"] = results_df[\"text\"]\n",
    "\n",
    "    # Add predicted labels to the combined DataFrame\n",
    "    all_results_df[f\"{column_prefix}_prediction\"] = results_df[\"predicted_label\"]\n",
    "\n",
    "logging.info(f\"Combined results DataFrame shape: {all_results_df.shape}\")\n",
    "\n",
    "# Sort columns for better readability\n",
    "all_results_df = all_results_df[\n",
    "    [\"custom_id\", \"original_label\", \"original_prediction\"]\n",
    "    + sorted(\n",
    "        [\n",
    "            col\n",
    "            for col in all_results_df.columns\n",
    "            if col not in [\"custom_id\", \"original_label\", \"original_prediction\", \"original_text\"]\n",
    "        ]\n",
    "    )\n",
    "    + [\"original_text\"]\n",
    "]\n",
    "\n",
    "# Save combined results to CSV\n",
    "combined_results_path = os.path.join(RESULTS_PATH, combined_results_file)\n",
    "all_results_df.to_csv(combined_results_path, index=False)\n",
    "logging.info(f\"Saved combined results to {combined_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2114d",
   "metadata": {},
   "source": [
    "### Analyse Comparisons and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3855cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:43:00,956 - INFO - Prediction columns found: ['claim_normalization_gpt-5-nano_prediction', 'claim_normalization_gpt-5_prediction']\n",
      "2025-12-03 11:43:00,972 - INFO - Saved comparison report to ../data/faketweetbr/classification_results/xlm-roberta-large/comparison_report.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize report dictionary\n",
    "report = {\n",
    "\t\"dataset_name\": DATASET_NAME,\n",
    "\t\"model_name\": MODEL_NAME.split(\"/\")[-1],\n",
    "}\n",
    "\n",
    "# Retrieve list of prediction columns\n",
    "prediction_columns = [col for col in all_results_df.columns if col.endswith(\"_prediction\") and not col.startswith(\"original\")]\n",
    "logging.info(f\"Prediction columns found: {prediction_columns}\")\n",
    "\n",
    "# Build analyses for each prediction column\n",
    "for col in prediction_columns:\n",
    "\t\tfor answer in [\"correct\", \"incorrect\"]:\n",
    "\t\t\t\tanalysis_name = f\"{answer}_after_{col}\"\n",
    "\t\t\t\treport[analysis_name] = {\n",
    "\t\t\t\t\t\t\"total\": 0,\n",
    "\t\t\t\t\t\t\"occurrences\": [],\n",
    "\t\t\t\t}\n",
    "  \n",
    "# Analyze each row in the combined results DataFrame\n",
    "for _, row in all_results_df.iterrows():\n",
    "\t\toriginal_pred = row[\"original_prediction\"]\n",
    "\t\toriginal_label = row[\"original_label\"]\n",
    "\n",
    "\t\tfor col in prediction_columns:\n",
    "\t\t\t\tmodel_pred = row[col]\n",
    "\n",
    "\t\t\t\tif original_pred != original_label:  # Original was incorrect\n",
    "\t\t\t\t\t\tif model_pred == original_label:  # Now correct\n",
    "\t\t\t\t\t\t\t\treport[f\"correct_after_{col}\"][\"total\"] += 1\n",
    "\t\t\t\t\t\t\t\treport[f\"correct_after_{col}\"][\"occurrences\"].append(row.to_dict())\n",
    "\t\t\t\telse:  # Original was correct\n",
    "\t\t\t\t\t\tif model_pred != original_label:  # Now incorrect\n",
    "\t\t\t\t\t\t\t\treport[f\"incorrect_after_{col}\"][\"total\"] += 1\n",
    "\t\t\t\t\t\t\t\treport[f\"incorrect_after_{col}\"][\"occurrences\"].append(row.to_dict())\n",
    "\n",
    "# Delete report entries with zero total\n",
    "for key in list(report.keys()):\n",
    "\t\tif key.startswith(\"correct_after_\") or key.startswith(\"incorrect_after_\"):\n",
    "\t\t\t\tif report[key][\"total\"] == 0:\n",
    "\t\t\t\t\t\tdel report[key]\n",
    "\n",
    "# Save report to a JSON file\n",
    "report_path = os.path.join(RESULTS_PATH, \"comparison_report.json\")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(report, f, indent=4)\n",
    "\t\t\n",
    "logging.info(f\"Saved comparison report to {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
